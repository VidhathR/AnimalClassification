{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da6a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the path to the main folder\n",
    "path_to_folder = \"Dataset\"\n",
    "\n",
    "# Loop through all subdirectories inside the main folder\n",
    "for subdir in os.listdir(path_to_folder):\n",
    "\n",
    "    # Define the path to the subdirectory\n",
    "    subdir_path = os.path.join(path_to_folder, subdir)\n",
    "\n",
    "    # Check if the subdirectory is a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "\n",
    "        # Loop through all image files inside the subdirectory\n",
    "        for file in os.listdir(subdir_path):\n",
    "\n",
    "            # Define the path to the image file\n",
    "            file_path = os.path.join(subdir_path, file)\n",
    "\n",
    "            # Check if the file is an image file\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "\n",
    "                # Read the image file\n",
    "                img = cv2.imread(file_path)\n",
    "\n",
    "                # Apply background removal using the GrabCut algorithm\n",
    "                # ...\n",
    "                mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "                # Define the rectangle containing the object you want to keep\n",
    "                rect = (50,50,450,290)\n",
    "\n",
    "                # Initialize the background and foreground models using the rectangle\n",
    "                bgdModel = np.zeros((1, 65), np.float64)\n",
    "                fgdModel = np.zeros((1, 65), np.float64)\n",
    "            \n",
    "                # Apply the GrabCut algorithm to the image to obtain the new mask\n",
    "                cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "                # Create a new mask with only the foreground pixels set to 1\n",
    "                new_mask = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')\n",
    "\n",
    "                # Apply the new mask to the original image\n",
    "                result = cv2.bitwise_and(img, img, mask=new_mask)\n",
    "\n",
    "                # Save the result\n",
    "                cv2.imwrite(file_path, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a17457",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54401699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85902 images belonging to 158 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_datagen.flow_from_directory('Dataset',target_size = (224,224), batch_size = 32,class_mode = 'categorical',subset = 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb2946ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21405 images belonging to 158 classes.\n"
     ]
    }
   ],
   "source": [
    "test = train_datagen.flow_from_directory('Dataset',target_size = (224,224), batch_size = 32, class_mode = 'categorical',subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a35a531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8d85be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = []\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'Dataset'\n",
    "\n",
    "# Get a list of all folders in the directory\n",
    "folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]\n",
    "\n",
    "# Print the folder names\n",
    "for folder in folders:\n",
    "    class_names.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba937667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 157, 157, 157])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a67735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d6eaaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,158):\n",
    "    num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7096a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(class_names)):\n",
    "    if class_names[i].startswith(\"n\"):\n",
    "        class_names[i] = class_names[i][10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8ae523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,60):\n",
    "    class_names[i] = class_names[i][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea213e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b5b0e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black_footed_Albatross',\n",
       " 'Laysan_Albatross',\n",
       " 'Sooty_Albatross',\n",
       " 'Groove_billed_Ani',\n",
       " 'Crested_Auklet',\n",
       " 'Least_Auklet',\n",
       " 'Parakeet_Auklet',\n",
       " 'Rhinoceros_Auklet',\n",
       " 'Brewer_Blackbird',\n",
       " 'Red_winged_Blackbird',\n",
       " 'Rusty_Blackbird',\n",
       " 'Yellow_headed_Blackbird',\n",
       " 'Bobolink',\n",
       " 'Indigo_Bunting',\n",
       " 'Lazuli_Bunting',\n",
       " 'Painted_Bunting',\n",
       " 'Cardinal',\n",
       " 'Spotted_Catbird',\n",
       " 'Gray_Catbird',\n",
       " 'Yellow_breasted_Chat',\n",
       " 'Eastern_Towhee',\n",
       " 'Chuck_will_Widow',\n",
       " 'Brandt_Cormorant',\n",
       " 'Red_faced_Cormorant',\n",
       " 'Pelagic_Cormorant',\n",
       " 'Bronzed_Cowbird',\n",
       " 'Shiny_Cowbird',\n",
       " 'Brown_Creeper',\n",
       " 'American_Crow',\n",
       " 'Fish_Crow',\n",
       " 'Black_billed_Cuckoo',\n",
       " 'Mangrove_Cuckoo',\n",
       " 'Yellow_billed_Cuckoo',\n",
       " 'Gray_crowned_Rosy_Finch',\n",
       " 'Purple_Finch',\n",
       " 'Northern_Flicker',\n",
       " 'Acadian_Flycatcher',\n",
       " 'Great_Crested_Flycatcher',\n",
       " 'Least_Flycatcher',\n",
       " 'Olive_sided_Flycatcher',\n",
       " 'Scissor_tailed_Flycatcher',\n",
       " 'Vermilion_Flycatcher',\n",
       " 'Yellow_bellied_Flycatcher',\n",
       " 'Frigatebird',\n",
       " 'Northern_Fulmar',\n",
       " 'Gadwall',\n",
       " 'American_Goldfinch',\n",
       " 'European_Goldfinch',\n",
       " 'Boat_tailed_Grackle',\n",
       " 'Eared_Grebe',\n",
       " 'Horned_Grebe',\n",
       " 'Pied_billed_Grebe',\n",
       " 'Western_Grebe',\n",
       " 'Blue_Grosbeak',\n",
       " 'Evening_Grosbeak',\n",
       " 'Pine_Grosbeak',\n",
       " 'Rose_breasted_Grosbeak',\n",
       " 'Pigeon_Guillemot',\n",
       " 'California_Gull',\n",
       " 'Glaucous_winged_Gull',\n",
       " 'Abyssinian',\n",
       " 'American Bobtail',\n",
       " 'American Curl',\n",
       " 'American Shorthair',\n",
       " 'American Wirehair',\n",
       " 'Applehead Siamese',\n",
       " 'Balinese',\n",
       " 'Bengal',\n",
       " 'Birman',\n",
       " 'Bombay',\n",
       " 'British Shorthair',\n",
       " 'Burmese',\n",
       " 'Burmilla',\n",
       " 'Calico',\n",
       " 'Canadian Hairless',\n",
       " 'Chartreux',\n",
       " 'Chausie',\n",
       " 'Chinchilla',\n",
       " 'Cornish Rex',\n",
       " 'Cymric',\n",
       " 'Devon Rex',\n",
       " 'Dilute Calico',\n",
       " 'Dilute Tortoiseshell',\n",
       " 'Domestic Long Hair',\n",
       " 'Domestic Medium Hair',\n",
       " 'Domestic Short Hair',\n",
       " 'Egyptian Mau',\n",
       " 'Exotic Shorthair',\n",
       " 'Extra-Toes Cat - Hemingway Polydactyl',\n",
       " 'Havana',\n",
       " 'Himalayan',\n",
       " 'Japanese Bobtail',\n",
       " 'Javanese',\n",
       " 'Korat',\n",
       " 'LaPerm',\n",
       " 'Maine Coon',\n",
       " 'Manx',\n",
       " 'Munchkin',\n",
       " 'Chihuahua',\n",
       " 'Japanese_spaniel',\n",
       " 'Maltese_dog',\n",
       " 'Pekinese',\n",
       " 'Shih-Tzu',\n",
       " 'Blenheim_spaniel',\n",
       " 'papillon',\n",
       " 'toy_terrier',\n",
       " 'Rhodesian_ridgeback',\n",
       " 'Afghan_hound',\n",
       " 'basset',\n",
       " 'beagle',\n",
       " 'bloodhound',\n",
       " 'bluetick',\n",
       " 'Walker_hound',\n",
       " 'English_foxhound',\n",
       " 'redbone',\n",
       " 'borzoi',\n",
       " 'Irish_wolfhound',\n",
       " 'Italian_greyhound',\n",
       " 'whippet',\n",
       " 'Ibizan_hound',\n",
       " 'Norwegian_elkhound',\n",
       " 'otterhound',\n",
       " 'Saluki',\n",
       " 'Scottish_deerhound',\n",
       " 'Weimaraner',\n",
       " 'Staffordshire_bullterrier',\n",
       " 'American_Staffordshire_terrier',\n",
       " 'Bedlington_terrier',\n",
       " 'Border_terrier',\n",
       " 'Kerry_blue_terrier',\n",
       " 'Irish_terrier',\n",
       " 'Norfolk_terrier',\n",
       " 'Norwich_terrier',\n",
       " 'Yorkshire_terrier',\n",
       " 'wire-haired_fox_terrier',\n",
       " 'Lakeland_terrier',\n",
       " 'Sealyham_terrier',\n",
       " 'Airedale',\n",
       " 'cairn',\n",
       " 'Australian_terrier',\n",
       " 'Dandie_Dinmont',\n",
       " 'Boston_bull',\n",
       " 'miniature_schnauzer',\n",
       " 'giant_schnauzer',\n",
       " 'standard_schnauzer',\n",
       " 'Scotch_terrier',\n",
       " 'Tibetan_terrier',\n",
       " 'silky_terrier',\n",
       " 'soft-coated_wheaten_terrier',\n",
       " 'West_Highland_white_terrier',\n",
       " 'Lhasa',\n",
       " 'flat-coated_retriever',\n",
       " 'curly-coated_retriever',\n",
       " 'golden_retriever',\n",
       " 'Labrador_retriever',\n",
       " 'Chesapeake_Bay_retriever',\n",
       " 'German_short-haired_pointer',\n",
       " 'vizsla']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d31fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_map = dict(zip(num,class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bd38f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laysan_Albatross'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ce00c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "3/3 [==============================] - 3s 681ms/step - loss: 4.6976 - accuracy: 0.0208\n",
      "Epoch 2/9\n",
      "3/3 [==============================] - 2s 672ms/step - loss: 4.3492 - accuracy: 0.2500\n",
      "Epoch 3/9\n",
      "3/3 [==============================] - 2s 672ms/step - loss: 3.9858 - accuracy: 0.5625\n",
      "Epoch 4/9\n",
      "3/3 [==============================] - 2s 667ms/step - loss: 4.0741 - accuracy: 0.4688\n",
      "Epoch 5/9\n",
      "3/3 [==============================] - 2s 673ms/step - loss: 4.1703 - accuracy: 0.5104\n",
      "Epoch 6/9\n",
      "3/3 [==============================] - 2s 684ms/step - loss: 3.9826 - accuracy: 0.4583\n",
      "Epoch 7/9\n",
      "3/3 [==============================] - 2s 675ms/step - loss: 3.7348 - accuracy: 0.5104\n",
      "Epoch 8/9\n",
      "3/3 [==============================] - 2s 694ms/step - loss: 3.8618 - accuracy: 0.4167\n",
      "Epoch 9/9\n",
      "3/3 [==============================] - 2s 670ms/step - loss: 3.0226 - accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d15c167dc0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set the input shape and number of classes\n",
    "img_height, img_width = 224, 224\n",
    "num_classes = 158\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train,steps_per_epoch = 3,epochs= 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "67e6f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669/669 [==============================] - 223s 334ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4954449894884373"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(test)\n",
    "r = np.argmax(result, axis=-1)\n",
    "mc.accuracy_score(test.classes,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1294c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094d4dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 467 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_datagen.flow_from_directory('test',target_size = (112,112), batch_size = 32,class_mode = 'categorical',subset = 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb46347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test = train_datagen.flow_from_directory('test',target_size = (112,112), batch_size = 32, class_mode = 'categorical',subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6f8858bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 3s 190ms/step - loss: 2.3106 - accuracy: 0.0851 - val_loss: 2.2987 - val_accuracy: 0.1043\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 2s 175ms/step - loss: 2.3026 - accuracy: 0.1034 - val_loss: 2.2968 - val_accuracy: 0.1043\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 2.2954 - accuracy: 0.1287 - val_loss: 2.2817 - val_accuracy: 0.1043\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 2.2960 - accuracy: 0.1264 - val_loss: 2.2845 - val_accuracy: 0.2435\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 2.2608 - accuracy: 0.1839 - val_loss: 2.1544 - val_accuracy: 0.2609\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 2.1603 - accuracy: 0.1862 - val_loss: 2.1608 - val_accuracy: 0.1826\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 2s 177ms/step - loss: 2.1437 - accuracy: 0.2299 - val_loss: 2.0639 - val_accuracy: 0.2174\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 2.0377 - accuracy: 0.2207 - val_loss: 2.0191 - val_accuracy: 0.2261\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 1.9897 - accuracy: 0.2506 - val_loss: 1.9894 - val_accuracy: 0.2261\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.9599 - accuracy: 0.2828 - val_loss: 1.9016 - val_accuracy: 0.3217\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.8947 - accuracy: 0.2805 - val_loss: 1.9408 - val_accuracy: 0.3217\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 1.8581 - accuracy: 0.3448 - val_loss: 1.9112 - val_accuracy: 0.2870\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.7324 - accuracy: 0.3287 - val_loss: 1.8285 - val_accuracy: 0.3304\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 2s 177ms/step - loss: 1.7701 - accuracy: 0.3586 - val_loss: 1.7993 - val_accuracy: 0.3478\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.6619 - accuracy: 0.4092 - val_loss: 1.7250 - val_accuracy: 0.4000\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.6649 - accuracy: 0.3954 - val_loss: 1.6607 - val_accuracy: 0.4087\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 2s 178ms/step - loss: 1.6552 - accuracy: 0.3724 - val_loss: 1.7513 - val_accuracy: 0.3652\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.5590 - accuracy: 0.4276 - val_loss: 1.6322 - val_accuracy: 0.4348\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 1.6136 - accuracy: 0.4023 - val_loss: 1.6521 - val_accuracy: 0.4261\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.5175 - accuracy: 0.4299 - val_loss: 1.6471 - val_accuracy: 0.3913\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.4979 - accuracy: 0.4759 - val_loss: 1.5241 - val_accuracy: 0.4609\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 1.5418 - accuracy: 0.4759 - val_loss: 1.6538 - val_accuracy: 0.4087\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 3s 180ms/step - loss: 1.4945 - accuracy: 0.4874 - val_loss: 1.5500 - val_accuracy: 0.4435\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 1.4041 - accuracy: 0.4644 - val_loss: 1.4688 - val_accuracy: 0.4783\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.3337 - accuracy: 0.5172 - val_loss: 1.3849 - val_accuracy: 0.5217\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 3s 199ms/step - loss: 1.3736 - accuracy: 0.4851 - val_loss: 1.3764 - val_accuracy: 0.5130\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 3s 243ms/step - loss: 1.3262 - accuracy: 0.5379 - val_loss: 1.4224 - val_accuracy: 0.4522\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 1.2858 - accuracy: 0.5333 - val_loss: 1.4135 - val_accuracy: 0.5043\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 3s 190ms/step - loss: 1.3142 - accuracy: 0.5448 - val_loss: 1.3676 - val_accuracy: 0.5043\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 2s 177ms/step - loss: 1.2167 - accuracy: 0.5513 - val_loss: 1.2542 - val_accuracy: 0.5391\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 1.2274 - accuracy: 0.5471 - val_loss: 1.2673 - val_accuracy: 0.5826\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 2s 175ms/step - loss: 1.0807 - accuracy: 0.5609 - val_loss: 1.3393 - val_accuracy: 0.5217\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.1896 - accuracy: 0.5402 - val_loss: 1.2673 - val_accuracy: 0.5304\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.0638 - accuracy: 0.5977 - val_loss: 1.1158 - val_accuracy: 0.5217\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 2s 171ms/step - loss: 1.0503 - accuracy: 0.6414 - val_loss: 1.2405 - val_accuracy: 0.5565\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 2s 178ms/step - loss: 1.0602 - accuracy: 0.6000 - val_loss: 1.1821 - val_accuracy: 0.5739\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.9603 - accuracy: 0.6230 - val_loss: 1.1518 - val_accuracy: 0.6348\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 1.0723 - accuracy: 0.6253 - val_loss: 1.2400 - val_accuracy: 0.5043\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 2s 172ms/step - loss: 0.9938 - accuracy: 0.6575 - val_loss: 1.1840 - val_accuracy: 0.6000\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 0.9851 - accuracy: 0.6368 - val_loss: 1.3012 - val_accuracy: 0.5565\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 1.0132 - accuracy: 0.6299 - val_loss: 1.1513 - val_accuracy: 0.5913\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 0.8917 - accuracy: 0.6759 - val_loss: 1.0408 - val_accuracy: 0.6261\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.8993 - accuracy: 0.6621 - val_loss: 1.0323 - val_accuracy: 0.6087\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 1.0445 - accuracy: 0.6437 - val_loss: 1.1426 - val_accuracy: 0.5739\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 2s 175ms/step - loss: 0.8304 - accuracy: 0.7103 - val_loss: 1.1341 - val_accuracy: 0.5913\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.8261 - accuracy: 0.6920 - val_loss: 1.2589 - val_accuracy: 0.5913\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 0.8245 - accuracy: 0.7195 - val_loss: 1.0159 - val_accuracy: 0.6000\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 2s 174ms/step - loss: 0.7842 - accuracy: 0.7080 - val_loss: 1.0882 - val_accuracy: 0.6348\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 2s 175ms/step - loss: 0.7594 - accuracy: 0.7241 - val_loss: 1.0045 - val_accuracy: 0.6609\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 2s 173ms/step - loss: 0.6942 - accuracy: 0.7402 - val_loss: 1.0795 - val_accuracy: 0.6522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2928aaed070>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Set the input shape and number of classes\n",
    "img_height, img_width = 112, 112\n",
    "num_classes = 10\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(train,steps_per_epoch = 467//32,epochs= 50,batch_size = 32,validation_data = test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4e1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Set the input shape and number of classes\n",
    "img_height, img_width = 112, 112\n",
    "num_classes = 10\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5083c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 110, 110, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 55, 55, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 53, 53, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 26, 26, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 688,938\n",
      "Trainable params: 688,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
